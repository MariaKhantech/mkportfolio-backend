
#see 
# https://python.plainenglish.io/train-a-custom-chatgpt-to-answer-questions-on-your-resume-5fde3ea128a0
#
# like the documentation we cn try it locally first. So your front end would call this API 
# (e.g localhost:8000/chat) and the API would call the OpenAI API to get the response.
# the /chat part should run the app.get("/chat") function and return the response from the OpenAI API.


## STEP1: you need to install these packages##
# pip install openai
# pip install gpt_index==0.4.24
# pip install PyPDF2
# pip install PyCryptodome
# pip install gradio

##


from fastapi import FastAPI
from mangum import Mangum
#STEP2: import the openai and other packages as seen in the documentation above
from gpt_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain.chat_models import ChatOpenAI
import gradio as gr
import sys
import os
from tqdm import tqdm

app = FastAPI()

#----STEP3: LOAD your openai key----#
# --- I created a new jey for you call chat key ---- #
os.environ["OPENAI_API_KEY"] = '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'

#----STEP4: create a function to create_index----#
def create_index(directory_path):
    
    # Path to the index file
    index_file_path = 'index.json'
    
    # Check if the index file already exists
    if os.path.exists(index_file_path):
        print(f"Index file {index_file_path} already exists.")
        # Optionally, load and return the existing index instead of exiting
        # index = GPTSimpleVectorIndex.load_from_disk(index_file_path)
        # return index
        return
    
    max_input_len = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_len, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name="gpt-4", max_tokens=num_outputs))
    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name="text-davinci-004", max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    
    # Save index to disk
    try:
        index.save_to_disk('index.json')
        print("Index saved to disk")
    except Exception as e:
        print(f"Error saving index to disk: {e}")

    return index

# 4.1 The create_index function uses the imported modules to create an index of documents in a specified directory path,
# which is used by the chatbot to generate responses.

# 4.2 you need to create a directory with the name  "docs" and put your resume in it.

# 4.3 The create_index function takes in the path to the directory containing the resume and returns the index.
# The index is a dictionary with the key being the name of the resume and the value being the text of the resume.
# The index is used by the chatbot to generate responses.
# The index is stored in a file called index.json in the same directory as the "docs".


#---STEP5: create a function called 'chat' to get the response---#
def chat(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode="tree_summarize")
    print(response.response)
    return response.response


#the chat function takes in a question and returns the response from the OpenAI API.
#Its the index.json file that is used to generate the response.
#the index.json file is created by the create_index function, which is generated by openai API.


@app.get("/chat")
def read_root():
    
    #---STEP6: call the create_index function---#
    create_index('docs')
    chat("what is my name")
    
    
    #---STEP7: call the chat function---#
    #the chat function takes in a question and returns the response from the OpenAI API.
    #so this will finish with a return statement so the response can be sent to the front end.
    #replace the return statement below with the response from the OpenAI API.
    #use a hard coded questions when passing in a parameter in the function for now to test the API i
    return {"data": "Hello World"}


handler = Mangum(app)

